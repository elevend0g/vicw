# -----------------------------
# LLM CONNECTION (The Critical Fix)
# -----------------------------
# Paste your actual key starting with 'sk-or-...'
VICW_LLM_API_KEY=sk-or-v1-***************************

# This is the fix for [Errno -2]. 
# Since this is a public URL, Docker can find it.
VICW_LLM_API_URL=https://openrouter.ai/api/v1/chat/completions

# The model you want to use on OpenRouter
VICW_LLM_MODEL_NAME=x-ai/grok-4.1-fast

# -----------------------------
# DATABASE SETTINGS
# -----------------------------
# Default password matches docker-compose defaults
NEO4J_PASSWORD=password

# -----------------------------
# ADVANCED TUNING (Optional)
# -----------------------------
# How many tokens to keep in memory before summarization kicks in
MAX_CONTEXT_TOKENS=16384

# When to offload context to the database (0.80 = 80% full)
OFFLOAD_THRESHOLD=0.80

# Maximum tokens for LLM response
LLM_MAX_TOKENS=4096

# RAG retrieval threshold (lower = more permissive, higher = more strict)
# Range: 0.0-1.0 (cosine similarity score)
RAG_SCORE_THRESHOLD=0.2

# Proactive Embedding (NEW)
# Enable eager background embedding of large messages
PROACTIVE_EMBED_ENABLED=true
# Minimum token count to trigger proactive embedding (default: 500)
PROACTIVE_EMBED_THRESHOLD=500