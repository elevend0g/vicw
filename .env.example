# -----------------------------
# LLM CONNECTION (The Critical Fix)
# -----------------------------
# Paste your actual key starting with 'sk-or-...'
VICW_LLM_API_KEY=sk-or-v1-c**************8

# This is the fix for [Errno -2]. 
# Since this is a public URL, Docker can find it.
VICW_LLM_API_URL=https://openrouter.ai/api/v1/chat/completions

# The model you want to use on OpenRouter
VICW_LLM_MODEL_NAME=anthropic/claude-haiku-4.5

# -----------------------------
# DATABASE SETTINGS
# -----------------------------
# Default password matches docker-compose defaults
NEO4J_PASSWORD=password

# -----------------------------
# ADVANCED TUNING (Optional)
# -----------------------------
# How many tokens to keep in memory before summarization kicks in
MAX_CONTEXT_TOKENS=16384

# When to offload context to the database (0.80 = 80% full)
OFFLOAD_THRESHOLD=0.80

# Maximum tokens for LLM response
LLM_MAX_TOKENS=8192

# Response format - use {"type": "text"} for plain text (no markdown headings)
# or {"type": "json_object"} to force JSON output
LLM_RESPONSE_FORMAT={"type": "text"}